{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import math\n",
    "import fasttext\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Facebook FastText Pre-trained Model\n",
    "model = fasttext.load_model('wiki.en.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Definitions\n",
    "#### Preprocesing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    # Read in CSV File into DataFrame\n",
    "    list_var = []\n",
    "    data = pd.DataFrame()\n",
    "    with open(filename,'r') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for i in reader:\n",
    "            list_var.append(i)\n",
    "    # Header & Data\n",
    "    col_name = list_var[0]\n",
    "    data_val = list_var[1:]\n",
    "    # Convert to DataFrame\n",
    "    data = pd.DataFrame(data_val, columns=col_name)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data,clean):\n",
    "    # Clean and Tokenize both Name and Description Field\n",
    "    temp_name = data['name'].copy()\n",
    "    temp_description = data['description'].copy()\n",
    "    df = data.copy()\n",
    "    # Raw\n",
    "    if clean=='Raw':\n",
    "        pass\n",
    "    # Semi-Cleaned \n",
    "    elif clean=='Nspace':\n",
    "        # Remove Symbols in Name and Description Column (no space)\n",
    "        temp_name = [re.sub(r'[^\\w\\s]','',x) for x in temp_name]\n",
    "        temp_description = [re.sub(r'[^\\w\\s]','',x) for x in temp_description]\n",
    "    elif clean=='Wspace':\n",
    "        # Remove Symbols in Name and Description Column (replace with space)\n",
    "        temp_name = [re.sub(r'[^\\w\\s]',' ',x) for x in temp_name]\n",
    "        temp_description = [re.sub(r'[^\\w\\s]',' ',x) for x in temp_description]\n",
    "    # Tokenize: Split single sentence into list of words\n",
    "    for i in range(0,len(temp_name),1):\n",
    "        # Tokenize Name and Description Columns into words\n",
    "        df['name'][i] = word_tokenize(temp_name[i])\n",
    "        df['description'][i] = word_tokenize(temp_description[i])\n",
    "        try:\n",
    "            df['name'][i].remove('')\n",
    "        except ValueError:\n",
    "            pass  # do nothing!\n",
    "        try:\n",
    "            df['description'][i].remove('')\n",
    "        except ValueError:\n",
    "            pass  # do nothing!\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(df):\n",
    "    # Vectorize the list of words and then average the vectors into one sentence vector\n",
    "    df_vectorize = df.copy()\n",
    "    counter = 0\n",
    "    arr = []\n",
    "    for i in df['name']:\n",
    "        array = [model.get_word_vector(x) for x in i]\n",
    "        avg = np.average(array, axis=0)\n",
    "        arr.append(avg)\n",
    "        counter += 1\n",
    "    df_vectorize.drop('name',axis=1)\n",
    "    df_vectorize['name'] = arr\n",
    "    #df_concat = pd.concat([df.id,df_vectorize.name,df.price,df.manufacturer],axis=1)\n",
    "    return df_vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_concat(df1,df2):\n",
    "    # Concate 2 word vectors from 2 model into a single word vector of dim 600\n",
    "    df = df1.copy()\n",
    "    arr = []\n",
    "    for i in range(0,len(df1),1):\n",
    "        vec1 = df1.name[i]\n",
    "        vec2 = df2.name[i]\n",
    "        comb = np.concatenate((vec1,vec2),axis=0)\n",
    "        arr.append(comb)\n",
    "    df['name'] = arr\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_vector(file,clean):\n",
    "    # Read in Abt File\n",
    "    df = read_file(file)\n",
    "    if 'title' in df.columns:\n",
    "        df['name']=df['title'].copy()\n",
    "    # Clean & Tokenize\n",
    "    temp = tokenize(df,clean)\n",
    "    df.name = temp.name\n",
    "    # Clean manufacture field \n",
    "    if 'manufacturer' not in df.columns:\n",
    "        manufact = []\n",
    "        for i in range(0,len(df),1):\n",
    "            manufact.append(df.name[i][0])\n",
    "        # Insert back in orignal df    \n",
    "        df['manufacturer'] = [i.lower() for i in manufact]\n",
    "    else:\n",
    "        df['manufacturer'] = [i.lower() for i in df.manufacturer] \n",
    "    # Convert Price into Float\n",
    "    arr = []\n",
    "    for i in df.price:\n",
    "        if i != '':\n",
    "            arr.append(float(i.replace('$','').replace(',','').replace('gbp','')))\n",
    "        else:\n",
    "            arr.append(float(0))\n",
    "    df['price'] = arr\n",
    "    # Vectorize name column\n",
    "    data = vectorize(df)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(df,df2,clean):\n",
    "    # Change Column names to 'name' instead of 'title'\n",
    "    if 'title' in df.columns:\n",
    "        df['name']=df['title'].copy()\n",
    "    if 'title' in df2.columns:\n",
    "        df2['name']=df2['title'].copy()  \n",
    "    # Combine name and description text\n",
    "    df['combine'] = df['name'] + ' ' + df['description']\n",
    "    df2['combine'] = df2['name'] + ' ' + df2['description']\n",
    "    # Create Corpus to train TFIDF\n",
    "    # With option for Raw vs Cleaned\n",
    "    corpus = []\n",
    "    for i in range(0,len(df['combine']),1):\n",
    "        temp = df['combine'][i]\n",
    "        if clean=='Raw':\n",
    "            pass\n",
    "        elif clean=='Nspace':\n",
    "            temp = re.sub(r'[^\\w\\s]','',temp)\n",
    "        elif clean=='Wspace':\n",
    "            temp = re.sub(r'[^\\w\\s]',' ',temp)\n",
    "        corpus.append(temp)\n",
    "    for i in range(0,len(df2['combine']),1):\n",
    "        temp = df2['combine'][i]\n",
    "        if clean=='Raw':\n",
    "            pass\n",
    "        elif clean=='Nspace':\n",
    "            temp = re.sub(r'[^\\w\\s]','',temp)\n",
    "        elif clean=='Wspace':\n",
    "            temp = re.sub(r'[^\\w\\s]',' ',temp)\n",
    "        corpus.append(temp)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_vect(file1,file2,clean):\n",
    "    # Read in Files\n",
    "    df = read_file(file1)\n",
    "    df2 = read_file(file2)\n",
    "    corpus = create_corpus(df,df2,clean)\n",
    "    # Fit TF-IDF Model\n",
    "    vectorizer = TfidfVectorizer(max_features=300)\n",
    "    tfidf_model = vectorizer.fit(corpus)\n",
    "    # Generate TF-IDF Vectors for each product\n",
    "    arr = []\n",
    "    for i in range(0,len(df),1):\n",
    "        text = df['name'][i]\n",
    "        arr.append(tfidf_model.transform([text]).toarray()[0])\n",
    "    df['name'] = arr\n",
    "    arr = []\n",
    "    for i in range(0,len(df2),1):\n",
    "        text = df2['name'][i]\n",
    "        arr.append(tfidf_model.transform([text]).toarray()[0])\n",
    "    df2['name'] = arr\n",
    "    \n",
    "    return df[['id','name']], df2[['id','name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Ranked List Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_length(a):\n",
    "    return float(math.sqrt(sum(x*x for x in a)))\n",
    "def dot_product(a,b):\n",
    "    if len(a) == len(b):\n",
    "        return sum([x*y for (x,y) in zip(a,b)])\n",
    "    else:\n",
    "        return \"Vector Length Different\"\n",
    "def cos_similarity(a,b):\n",
    "    return dot_product(a,b)/(vector_length(a)*vector_length(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_ranking(df1,df2):\n",
    "    # Initialize Ranking Chart\n",
    "    col_names = ['df1_id','1','2','3','4','5','6','7','8','9','10']\n",
    "    df_ranking = pd.DataFrame(columns=col_names);\n",
    "    # Loop for every Abt_id element\n",
    "    for i in range(0,len(df1.name),1):\n",
    "        cos_sim = []\n",
    "        # Loop for computing buy_id ranking for current Abt_id\n",
    "        for j in range(0,len(df2.name),1):\n",
    "            # Find cos_similarity score between name pairs\n",
    "            cos_score = cos_similarity(df1.name[i],df2.name[j])\n",
    "            # If manufacture not empty\n",
    "            if(df1.manufacturer[i]!='') and (df2.manufacturer[j]!=''):\n",
    "                # If matching manufacture add to score\n",
    "                if (df1.manufacturer[i] in df2.manufacturer[j]) or (df2.manufacturer[j] in df1.manufacturer[i]):\n",
    "                    pass\n",
    "                # No match subtract score\n",
    "                else:\n",
    "                    cos_score = cos_score - 0.25\n",
    "            # If price not empty and not equal to zero\n",
    "            if(df1.price[i]!='') and (df2.price[j]!=''):\n",
    "                # If two price have percent differecence greater than 50% than subtract score\n",
    "                if(df1.price[i]!=0) and (df2.price[j]!=0): \n",
    "                    if df1.price[i] >= df2.price[j]:\n",
    "                        if (float(df1.price[i]-df2.price[j])/df1.price[i]) >= 0.50:\n",
    "                            cos_score = cos_score - 0.25\n",
    "                    else:\n",
    "                        if (float(df2.price[j]-df1.price[i])/df2.price[j]) >= 0.50:\n",
    "                            cos_score = cos_score - 0.25\n",
    "            # Save cos_similarity score to array\n",
    "            cos_sim.append(cos_score)\n",
    "        # Sort list and pick top 10\n",
    "        temp_df = pd.DataFrame(cos_sim)\n",
    "        temp_sorted = temp_df.sort_values(by=0,ascending=False)\n",
    "        row_ranking = temp_sorted.index.values[0:10]\n",
    "        rank_buy_id = []\n",
    "        # Convert row # into buy_id\n",
    "        for k in row_ranking:\n",
    "            rank_buy_id.append(df2.id.iloc[k])\n",
    "        rank_buy_id.insert(0,df1.id[i])\n",
    "        # Fill Rank ID to 10 rows \n",
    "        if len(rank_buy_id) <= 10:\n",
    "            for i in range(0,11-len(rank_buy_id),1):\n",
    "                rank_buy_id.insert(len(rank_buy_id),'')\n",
    "        # Concat into Abt Ranking Chart\n",
    "        combine_df = pd.DataFrame([rank_buy_id],columns=col_names)\n",
    "        df_ranking = pd.concat([df_ranking,combine_df],names=col_names,sort=False,ignore_index=True)\n",
    "    return df_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgcos_top_10_ranking(df1,df2,df3,df4):\n",
    "    # df1 & df2 from First Model\n",
    "    # df3 & df4 from Second Model\n",
    "    # Initialize Ranking Chart\n",
    "    col_names = ['df1_id','1','2','3','4','5','6','7','8','9','10']\n",
    "    df_ranking = pd.DataFrame(columns=col_names);\n",
    "    # Loop for every Abt_id element\n",
    "    for i in range(0,len(df1.name),1):\n",
    "        cos_sim = []\n",
    "        # Loop for computing buy_id ranking for current Abt_id\n",
    "        for j in range(0,len(df2.name),1):\n",
    "            # Compute Cosine Similarity for 2 models and average their cos sim\n",
    "            score1 = cos_similarity(df1.name[i],df2.name[j])\n",
    "            score2 = cos_similarity(df3.name[i],df4.name[j])\n",
    "            meanscore = (score1+score2)/2\n",
    "            cos_sim.append(meanscore)\n",
    "        # Sort list and pick top 10\n",
    "        temp_df = pd.DataFrame(cos_sim) \n",
    "        temp_sorted = temp_df.sort_values(by=0,ascending=False)\n",
    "        row_ranking = temp_sorted.index.values[0:10]\n",
    "        rank_buy_id = []\n",
    "        # Convert row # into buy_id\n",
    "        for k in row_ranking:\n",
    "            rank_buy_id.append(df2.id.iloc[k])\n",
    "        rank_buy_id.insert(0,df1.id[i])\n",
    "        # Concat into Abt Ranking Chart\n",
    "        combine_df = pd.DataFrame([rank_buy_id],columns=col_names)\n",
    "        df_ranking = pd.concat([df_ranking,combine_df],names=col_names,sort=False,ignore_index=True)\n",
    "    return df_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_ranking_loc(df_ranking,mapping,col1,col2):\n",
    "    # Find where ground truth is ranked \n",
    "    col_names = ['df1_id','Ranked_Location']\n",
    "    df_rank_loc = pd.DataFrame(columns=col_names)\n",
    "    # Loop for every element of Abt_id from df_ranking\n",
    "    for i in range(0,len(df_ranking.df1_id),1):\n",
    "        # Look for ground truth from True Mapping\n",
    "        look_up_id = df_ranking.df1_id[i]\n",
    "        # Check if product is in Perfect Mapping\n",
    "        if len(mapping[col2][mapping[col1]==look_up_id])!=0:\n",
    "            mapped_id = mapping[col2][mapping[col1]==look_up_id].values[0]\n",
    "            # Check if ground truth in ranking list and note its location\n",
    "            if mapped_id in list(df_ranking.iloc[i]):\n",
    "                ranking = list(df_ranking.iloc[i]).index(mapped_id)\n",
    "            else:\n",
    "                ranking = 0\n",
    "            # Concat into Abt Ranking Location df\n",
    "            temp_list = list([look_up_id,ranking])\n",
    "            temp_df2 = pd.DataFrame([temp_list],columns=col_names)\n",
    "            df_rank_loc = pd.concat([df_rank_loc,temp_df2],names=col_names,sort=False,ignore_index=True)\n",
    "    return df_rank_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_at_n(df_rank_loc,n):\n",
    "    # Sum the correct ranks at ranks 1 to n divide by n number of ranks\n",
    "    HR_n_sum = 0\n",
    "    for i in range(0,len(df_rank_loc),1):\n",
    "        if df_rank_loc.Ranked_Location[i]!=0:\n",
    "            if df_rank_loc.Ranked_Location[i]<=n:\n",
    "                HR_n_sum += 1\n",
    "    Hits_Rate_at_n = HR_n_sum/len(df_rank_loc)\n",
    "    return Hits_Rate_at_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(df_rank_loc):\n",
    "    # Sum the Reciprocal Rank of Abt Ranking Location df and divide by total number element\n",
    "    MRR_sum = 0\n",
    "    for i in range(0,len(df_rank_loc),1):\n",
    "        if df_rank_loc.Ranked_Location[i]!=0:\n",
    "            MRR_sum += 1/df_rank_loc.Ranked_Location[i]\n",
    "    mean_reciprocal_rank = MRR_sum/len(df_rank_loc)\n",
    "    return mean_reciprocal_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://gist.github.com/bwhite/3726239\n",
    "def dcg_at_k(r, k):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return np.sum(np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    idcg = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not idcg:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / idcg\n",
    "\n",
    "def ndcg(df_rank_loc,k):\n",
    "    # Sum up all NDCG at each Queries and divided by total # of Queries\n",
    "    # Initalize Ranking Encoding\n",
    "    rank_encoding = {0:[0,0,0,0,0,0,0,0,0,0],\n",
    "                     1:[1,0,0,0,0,0,0,0,0,0],\n",
    "                     2:[0,1,0,0,0,0,0,0,0,0],\n",
    "                     3:[0,0,1,0,0,0,0,0,0,0],\n",
    "                     4:[0,0,0,1,0,0,0,0,0,0],\n",
    "                     5:[0,0,0,0,1,0,0,0,0,0],\n",
    "                     6:[0,0,0,0,0,1,0,0,0,0],\n",
    "                     7:[0,0,0,0,0,0,1,0,0,0],\n",
    "                     8:[0,0,0,0,0,0,0,1,0,0],\n",
    "                     9:[0,0,0,0,0,0,0,0,1,0],\n",
    "                     10:[0,0,0,0,0,0,0,0,0,1]}\n",
    "    sum_NDCG = 0\n",
    "    for i in df_rank_loc.Ranked_Location:\n",
    "        sum_NDCG += ndcg_at_k(rank_encoding[i], k)\n",
    "    NDCG = sum_NDCG/len(df_rank_loc)\n",
    "    return NDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All in One Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_in_one(file1,file2,mapping,clean):\n",
    "    # Generate TF-IDF Vectors\n",
    "    data1_1, data2_1 = tf_idf_vect(file1,file2,clean)\n",
    "    # FastText Pre-Trained\n",
    "    # Read File 1 & Vectorize\n",
    "    data1_2 = file_to_vector(file1,clean)\n",
    "    # Read File 2 & Vectorize\n",
    "    data2_2 = file_to_vector(file2,clean)\n",
    "    # Concate word vectors from both models of file 1 into one word vector of dim 600\n",
    "    data1 = vec_concat(data1_2,data1_1)\n",
    "    # Concate word vectors from both models of file 2 into one word vector of dim 600\n",
    "    data2 = vec_concat(data2_2,data2_1)\n",
    "    # Read Perfect Mapping\n",
    "    mapping = read_file(mapping)\n",
    "    # Create Ranking List\n",
    "    df_ranking = top_10_ranking(data1,data2)\n",
    "    # Find Location where True Ranking Occured \n",
    "    df_rank_loc = true_ranking_loc(df_ranking,mapping,mapping.columns[0],mapping.columns[1])\n",
    "    return df_rank_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abt vs Buy (Raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw\n",
      "Hit Rate at 10:  0.9010175763182239\n",
      "Mean Reciprocal Rank:  0.67933681335624\n",
      "NDCG at 10 Score:  0.7330225625581259\n"
     ]
    }
   ],
   "source": [
    "df_rank_loc = all_in_one('Abt.csv','Buy.csv','abt_buy_perfectMapping.csv','Raw')\n",
    "print(\"Raw\")\n",
    "print(\"Hit Rate at 10: \", hits_at_n(df_rank_loc,10))\n",
    "print(\"Mean Reciprocal Rank: \", mean_reciprocal_rank(df_rank_loc))\n",
    "print('NDCG at 10 Score: ', ndcg(df_rank_loc,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abt vs Buy (Cleaned No Space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No space\n",
      "Hit Rate at 10:  0.910268270120259\n",
      "Mean Reciprocal Rank:  0.7021467483077102\n",
      "NDCG at 10 Score:  0.7526898239597221\n"
     ]
    }
   ],
   "source": [
    "df_rank_loc = all_in_one('Abt.csv','Buy.csv','abt_buy_perfectMapping.csv','Nspace')\n",
    "print(\"No space\")\n",
    "print(\"Hit Rate at 10: \", hits_at_n(df_rank_loc,10))\n",
    "print(\"Mean Reciprocal Rank: \", mean_reciprocal_rank(df_rank_loc))\n",
    "print('NDCG at 10 Score: ', ndcg(df_rank_loc,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abt vs Buy (Cleaned With Space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With space\n",
      "Hit Rate at 10:  0.849213691026827\n",
      "Mean Reciprocal Rank:  0.6245789465955979\n",
      "NDCG at 10 Score:  0.678885481384402\n"
     ]
    }
   ],
   "source": [
    "df_rank_loc = all_in_one('Abt.csv','Buy.csv','abt_buy_perfectMapping.csv','Wspace')\n",
    "print(\"With space\")\n",
    "print(\"Hit Rate at 10: \", hits_at_n(df_rank_loc,10))\n",
    "print(\"Mean Reciprocal Rank: \", mean_reciprocal_rank(df_rank_loc))\n",
    "print('NDCG at 10 Score: ', ndcg(df_rank_loc,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon vs Google (Raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw\n",
      "Hit Rate at 10:  0.8032345013477089\n",
      "Mean Reciprocal Rank:  0.6075311256578108\n",
      "NDCG at 10 Score:  0.6552468257693013\n"
     ]
    }
   ],
   "source": [
    "df_rank_loc = all_in_one('Amazon.csv','GoogleProducts.csv','Amzon_GoogleProducts_perfectMapping.csv','Raw')\n",
    "print(\"Raw\")\n",
    "print(\"Hit Rate at 10: \", hits_at_n(df_rank_loc,10))\n",
    "print(\"Mean Reciprocal Rank: \", mean_reciprocal_rank(df_rank_loc))\n",
    "print('NDCG at 10 Score: ', ndcg(df_rank_loc,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon vs Google (Cleaned No Space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No space\n",
      "Hit Rate at 10:  0.8571428571428571\n",
      "Mean Reciprocal Rank:  0.6593430453942589\n",
      "NDCG at 10 Score:  0.7077453673757608\n"
     ]
    }
   ],
   "source": [
    "df_rank_loc = all_in_one('Amazon.csv','GoogleProducts.csv','Amzon_GoogleProducts_perfectMapping.csv','Nspace')\n",
    "print(\"No space\")\n",
    "print(\"Hit Rate at 10: \", hits_at_n(df_rank_loc,10))\n",
    "print(\"Mean Reciprocal Rank: \", mean_reciprocal_rank(df_rank_loc))\n",
    "print('NDCG at 10 Score: ', ndcg(df_rank_loc,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon vs Google (Cleaned With Space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With space\n",
      "Hit Rate at 10:  0.8247978436657682\n",
      "Mean Reciprocal Rank:  0.64305359460346\n",
      "NDCG at 10 Score:  0.6876296949577001\n"
     ]
    }
   ],
   "source": [
    "df_rank_loc = all_in_one('Amazon.csv','GoogleProducts.csv','Amzon_GoogleProducts_perfectMapping.csv','Wspace')\n",
    "print(\"With space\")\n",
    "print(\"Hit Rate at 10: \", hits_at_n(df_rank_loc,10))\n",
    "print(\"Mean Reciprocal Rank: \", mean_reciprocal_rank(df_rank_loc))\n",
    "print('NDCG at 10 Score: ', ndcg(df_rank_loc,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### Using the TF-IDF + Pre-Trained (SkipGram) Vector Concatenation + Soft Blocking (-0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   Vector Concatenation    | Hits Rate at 10 | Mean Reciprocal Rank | NDCG Score at 10 |\n",
    "| ------------------------     | ---   | ---     |   --- |\n",
    "| Abt_buy (Raw)                         | 0.901 |  0.679  | 0.733 |\n",
    "| Abt_buy (Cleaned No Space)            | 0.910 |  0.702  | 0.753 |\n",
    "| Abt_buy (Cleaned With Space)          | 0.849 |  0.625  | 0.679 |\n",
    "| Amazon_Google (Raw)                   | 0.803 |  0.608  | 0.655 |\n",
    "| Amazon_Google (Cleaned No Space)      | 0.857 |  0.659  | 0.708 |\n",
    "| Amazon_Google (Cleaned With Space)    | 0.825 |  0.643  | 0.688 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
